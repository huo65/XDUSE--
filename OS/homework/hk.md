





# process&thread

1. **A computer system has enough room to hold five programs in its main memory. These programs are idle waiting for I/O half the time. What fraction of the CPU time is wasted?**

五个进程都处于等待 I/O 的空闲状态的概率是相等的，那么五个进程都处于等待 I/O 的空闲状态的概率为 1/32，CPU 空闲时间就是 1/32。

> 答案是1/32，但为什么？
>
> 句子 "这些程序有一半的时间是闲置的，在等待I/O "是含糊不清的。让我们看看这句话的几种不同解释方式，看看它们是否符合预期的答案：
>
> a) "这5个程序中的每一个都花了总时间的50%来等待IO"。在这种情况下，当一个程序在等待IO时，CPU可能被其他程序使用；而所有程序加起来可以使用100%的CPU时间，没有时间浪费。事实上，你只需要两个程序就可以使用100%的CPU时间（第一个程序使用CPU，而第二个程序等待IO，然后第二个程序使用CPU，而第一个任务等待IO，然后...）。这不可能是 "这些程序有一半的时间是空闲的，在等待I/O "的本意，因为答案（可能是零CPU时间的浪费）与预期的答案不一致。
>
> b) "所有的程序都在同一时间闲置，有一半的时间在等待I/O"。这不可能是问题的本意，因为答案显然是 "50%的CPU时间被浪费了"，与预期答案不符。
>
> c) "每个程序花费一半的时间来等待IO"。在这种情况下，第一个程序有100%的CPU可用时间，但花了50%的时间使用CPU，另外50%的时间等待IO，剩下50%的CPU时间可供下一个程序使用；然后第二个程序用50%的剩余CPU时间（总时间的25%）使用CPU，50%的剩余CPU时间（总时间的25%）等待IO，剩下25%的CPU时间可供下一个程序使用；然后第三个程序用50%的剩余CPU时间（12。 5%的总时间）使用CPU和50%的剩余CPU时间（总时间的12.5%）等待IO，剩下12.5%的CPU时间可供下一个程序使用，然后...
>
> 在这种情况下，剩余时间被每个程序减半，所以你得到一个 "2的负次方 "序列（1/2，1/4，1/8，1/16，1/32），得出的答案与预期的答案一致。
>
> 因为我们得到了这个解释的正确答案，所以我们可以假设这就是 "这些程序在一半的时间内闲置等待I/O "的含义。

2. **Assume that you are trying to download a large 2-GB file from the Internet. The file is available from a set of mirror servers, each of which can deliver a subset of the file's bytes; assume that a given request specifies the starting and ending bytes of the file. Explain how you might use threads to improve the download time.**
   - 将文件分成小块：在下载文件之前，将其分成小块。
   - 创建多个线程： 创建多个线程（每块文件一个），并指定每个线程从一个可用的镜像服务器上下载特定的文件块。
   - 同步下载文件： 由于每个线程都被指定下载一个特定的文件块，所有线程都可以同时下载文件，从而实现更快的下载。
   - 重新组合文件： 一旦所有的线程都完成了各自的文件块的下载，重新组合这些文件块以创建完整的文件。

用这种方法，你可以实现更快的下载时间，因为文件是同时从多个镜像服务器下载的。此外，这也有助于平衡各个服务器的负载，防止任何单一服务器成为下载过程中的瓶颈。

3. **In the text, we described a multithreaded Web server, showing why it is better than a single-threaded server and a finite-state machine server. Are there any circumstances in which a single-threaded server might be better? Give an example.**

是的，在某些情况下，单线程的服务器可能会更好。例如当服务器上的工作负荷很轻，传入的请求数量很低。在这种情况下，**创建和管理多个线程的开销**实际上可能会减慢对请求的处理。大于并发好处
eg：顺序访问（无TSL忙等待）

> 课本答案：是的。如果服务器是完全CPU绑定的，则不需要多线程。这只会增加不必要的复杂性。假设某个百万人口区域的电话查号系统(类似于114)，如果每个(姓名，电话号码)记录为64个字符，整个的数据库则为64MB，这就很容易全部读入服务器内存中以提供快速的查询

4. **Can a thread ever be preempted by a clock interrupt? If so, under what circumstances? If not, why not?**

用户级线程不能被时钟抢占，除非整个进程的量子被用完了（尽管透明的时钟中断可能发生）。
内核级线程可以被单独抢占。在后一种情况下，如果一个线程运行时间过长，时钟会中断当前进程，从而中断当前线程。内核可以自由地从同一进程中挑选一个不同的线程来运行，如果它愿意的话。

> 是的，在某些情况下，一个线程可以被时钟中断所抢占。
>
> 当一个线程在CPU上执行时，它拥有对CPU的控制权，直到它自愿将控制权交还给操作系统或被操作系统抢占为止。抢占可以通过两种方式发生：通过自愿让步或通过非自愿抢占。
>
> 当一个线程通过调用一个阻塞的系统调用或让位给另一个线程，明确地放弃对CPU的控制时，就会发生自愿让位。在这种情况下，线程自愿放弃对CPU的控制，而操作系统可以安排另一个线程执行。
>
> 非自愿抢占发生在操作系统决定抢占当前运行的线程并切换到另一个线程。这可能发生在一个更高优先级的线程准备运行时，或发生一个定时器中断（如时钟中断）时。
>
> 当定时器中断发生时，操作系统被通知，它可以选择抢占当前运行的线程并安排另一个线程运行。这使得操作系统能够确保没有线程在过长的时间内垄断CPU，并确保所有线程都有公平的执行机会。
>
> 因此，综上所述，如果操作系统决定安排另一个线程运行以响应中断，那么一个线程可以被时钟中断所抢占。

5. **What is the biggest advantage of implementing threads in user space? What is the biggest disadvantage?**

最大的优势就是效率。不需要陷入内核来切换线程。最大的缺点是，如果一个线程阻塞，整个进程都会阻塞。

> 在用户空间实现线程的最大优势是，它为程序员或应用程序开发人员提供了更多的灵活性和控制。用户级线程可以在不涉及操作系统的情况下实现，因此它们的开销较低，可以比内核级线程更快创建和切换。这意味着拥有大量线程的应用程序可以通过使用用户级线程实现更好的性能。
>
> 另一个优点是，**用户级线程不受底层操作系统的调度策略的限制**。应用程序开发人员可以实现他们自己的调度算法或同步机制，这可以根据应用程序的具体需求进行调整。这可以使应用程序具有更好的性能和可扩展性。
>
> 然而，用户级线程的最大缺点是不能利用多处理或多核处理器的优势。所有的用户级线程都在一个进程中运行，并共享同一个CPU，而内核级线程可以在多个CPU上调度，从而获得更大的吞吐量和性能。
>
> 此外，用户级线程可能无法与内核级资源互动，如I/O操作和系统调用，这可能会限制它们在某些类型的应用程序中的作用。最后，用户级线程不受内核内存管理系统的保护，这使得它们更容易受到错误和编程错误的影响，如内存泄漏，这可能导致整个程序崩溃

6. ==**When a computer is being developed, it is usually first simulated by a program that runs one instruction at a time. Even multiprocessors are simulated strictly sequentially like this. Is it possible for a race condition to occur when there are no simultaneous events like this?**==

是的，即使在顺序执行模型中一次执行一条指令，也有可能发生race condition。

//两个进程不会同时进入关键区

当多个线程或进程同时访问相同的共享资源时，如果没有适当的同步机制来确保相互排斥，就会出现race condition。在这种情况下，即使线程或进程是按顺序执行的，即一次一条指令，也可能出现race condition，因为指令的执行顺序和时间可能导致意外行为。**指令交错**

> 例如，在进程A运行时，它读取一些共享变量。然后发生了一个模拟时钟周期和进程B运行。它也读取相同的变量，然后对变量进行了加1操作。当进程A运行时，如果它也对变量进行了加1操作，

7. **If a system has only two processes, does it make sense to use a barrier to synchronizethem? Why or why not?**

 如果程序是分阶段运行的，在当前阶段结束之前，任何进程都不能进入下一阶段，那么就需要使用一个屏障。

> 它取决于两个进程的具体要求和依赖关系。
> 一般来说，屏障可以用来确保两个进程在执行到某一点时才进入下一步，从而使它们的行动同步。然而，如果两个进程之间没有依赖关系或相互作用，可以独立运行而不影响对方的结果，那么可能就没有必要使用障碍。
> 另一方面，如果两个进程之间有依赖关系或相互作用，比如需要共享资源或通信，那么可能需要使用屏障来确保它们的正确同步并避免潜在的冲突或错误。
>
> 通过使用屏障来同步进程，开发人员可以确保他们的程序顺利运行，没有错误，特别是在两个进程之间存在依赖关系的情况下。如果没有适当的同步，一个进程可能在另一个进程还在完成前一阶段的工作时就开始进行下一阶段的工作，从而导致程序运行中出现意外的结果和错误。

8. **Consider the following piece of C code:**

   **void main( ) {**

   **fork( );**

   **fork( );**

   **exit( );**

   **}**

   **How many child processes are created upon execution of this program**

三个子进程被创建。在最初的进程分叉之后，有两个进程在运行，一个是父进程，一个是子进程。此时有两个进程在运行，一个是父进程，一个是子进程。然后它们各自分叉，创建两个额外的进程。最后所有的进程都退出。

# IPC


**For the mutex implementation program below, please fill the blanks and then specify the initial value of LOCK, finally explain why the initial value of LOCK should be the one you give.**



![img](hk.assets/21a36b213d04063db5f60e7c817b3885.png)

![img](hk.assets/02c764d75b2925b791b9889dd12de5f8.png)

LOCK的初始值应为0。这是因为互斥锁有两种状态——锁定和未锁定。如果我们将LOCK的初始值设置为0，这意味着互斥锁最初处于解锁状态，允许任何线程获取锁。
详细地说，互斥锁函数使用测试和设置（TSL）指令将REGISTER.lock的值原子地设置为1，并返回其以前的值。然后，函数会检查上一个值是否已经为0——如果是，则意味着锁已成功获取，函数会返回。否则，它会调用thread_yield函数来放弃CPU并等待锁可用。
另一方面，互斥锁函数只需将LOCK的值设置回0即可释放锁，从而允许另一个线程获取锁。
因此，如果我们将LOCK的初始值设置为1，则意味着互斥锁最初被锁定，这将破坏最初拥有互斥锁的目的，因为在解锁之前，没有线程能够获取锁。因此，将初始值设置为0可以在线程之间进行适当的互斥同步。

**For the condition,"No process running outside its critical region may block other processes.", which is one of the conditions  to have a good solution for race condition/mutual exclusion, please explain why.**

临界区是多个进程共享的一段关键代码，进入这段代码的进程需要获得临界资源的访问权，而其他进程则需要等待该进程退出临界区后才能进入，以保证对临界资源的互斥访问。

如果一个进程在临界区之外阻塞了其他进程，则会导致其他进程无法获取临界资源的访问权，进而无法执行临界区代码，这就违反了临界区的互斥性原则。因此，在设计并发程序时，需要尽量避免在临界区之外造成进程的阻塞，以保证程序的正确性和高效性

# fork-exec

**Please write a program which will create 9 child processes by using "for" loop in C programming .** 、



在操作系统中，父进程和子进程是多进程编程的两个核心概念。父进程是创建其他进程的进程，而子进程是由父进程创建的新进程。

当一个进程fork()（派生）一个新进程时，子进程的创建过程会复制父进程的所有状态信息，并分配一个独立的进程ID。这意味着子进程拥有与父进程相同的代码和数据，但也可以通过exec()函数替换自己的程序代码和数据。在此之后，父进程和子进程一起运行，但在内核中它们是完全独立的进程，相互之间没有直接联系，通过进程间通信（IPC）机制才能实现进程之间的通讯和协作。

当父进程终止或被杀死时，子进程并不会随之终止，相反，子进程将成为操作系统中的一个孤儿进程，并被将其父ID更改为1（即init进程）。在某些情况下，父进程可以使用wait()函数等待子进程退出，并获得子进程的退出状态。进程树就是由父进程和其创建的所有子进程组成的层次结构。

因此，父进程和子进程的关系是一种层次关系，父进程是子进程的创建者，而子进程是父进程的一个派生进程，并且它们都可以有自己独立的执行过程和状态。同时他们也可以通过IPC机制进行通信和协作。

**What is process creation in an operating system, and how is it accomplished?**

在操作系统中，进程创建是指创建一个新的进程（子进程）以及为它分配资源和内存空间的过程。进程创建通常由父进程调用系统调用如fork()或exec()来完成。

在fork()函数调用时，操作系统会复制当前进程（即父进程）的所有状态信息（包括代码、数据、堆栈等）并创建一个新的进程（即子进程）。子进程独立于父进程运行，并按照与父进程相同的方式执行程序代码。不同的是，子进程拥有自己的进程标识符(process ID)，可以使用该标识符在操作系统中唯一地识别该进程。

在exec()函数调用时，进程会被新的程序代码和数据覆盖，从而成为一个全新的进程。这个过程中，进程的原始代码和数据都将被替换，但是它们的进程标识符和其他进程特性不会改变。

进程创建不仅涉及到复制进程的状态，还涉及到为新进程分配资源和内存空间。在Linux和Unix系统中，新进程将复制一份父进程的文件描述符表（file descriptor table），从而将父进程打开的文件等资源复制到子进程中。此外，新进程还将有一个新的内存空间，可以根据需要动态分配堆内存和栈空间。

总之，进程创建是指操作系统通过复制父进程的状态信息来创建一个新的进程。该过程涉及到分配资源和内存空间，并为子进程分配一个唯一的进程标识符。在完成进程创建之后，父进程和子进程就可以独立地运行并执行各自的程序代码。

**What is a zombie process, and how does the operating system handle it?**

在操作系统中，当一个进程已经完成了它的执行，但是它的父进程还没有调用wait()或waitpid()函数来获取退出状态信息时，该进程就会变成一个“僵尸进程”，也称为“zombie process”。

这是因为每个进程都应该有一个父进程，并且当一个进程终止时，它的父进程应该调用wait()或waitpid()函数来获取它的退出状态。如果父进程没有及时处理子进程退出的信息，子进程将保持在僵尸状态。

僵尸进程占用了系统内存，影响系统的性能，并且可能导致系统资源耗尽。要解决这个问题，操作系统会在发现有僵尸进程存在时，将其标记为“zombie”状态，并将其进程表项从系统中删除。此外，内核还会向父进程发送SIGCHLD信号通知其子进程已退出，并让父进程在适当的时候调用wait()或waitpid()函数来获取子进程的退出状态信息。

通过这种方式，操作系统可以有效地处理僵尸进程，防止它们占用太多资源并影响系统的正常运行。因此，父进程应该及时处理子进程退出状态信息，以避免产生僵尸进程，而操作系统也应该及时清理僵尸进程以保证系统的正常运行。

# PM&MM

**How does the operating system manage process termination, and what resources need to be deallocated?**

当进程终止时，操作系统会将该进程拥有的所有资源释放回系统。这包括内存、CPU时间、打开的文件和其他资源。
操作系统首先在进程表中将进程标记为“已终止”。然后，它向进程发送一个信号（如SIGTERM或SIGKILL），告诉它终止。如果进程中有任何打开的文件，操作系统会将其关闭。如果进程分配了任何内存，操作系统就会将其释放。如果进程持有任何锁或信号量，操作系统会释放它们。
最后，操作系统将进程从可运行进程的调度程序队列中删除，从而释放出它正在使用的任何CPU时间。进程表条目也被删除，使得诸如进程ID、用户ID和组ID之类的资源可以重用。
总的来说，操作系统负责释放被终止进程正在使用的所有资源，确保这些资源可供其他进程使用。

**How does the process manager handle memory access violations in an operating system?**

当进程试图访问未被授权访问的内存时，或者当它试图读取或写入无效的内存地址时，操作系统会将其检测为内存访问违规或分段故障。
操作系统中的进程管理器通常通过终止违规进程来处理内存访问违规。当操作系统检测到这样的违规行为时，它会向进程发送一个信号，指示它遇到了错误。进程的默认行为是在接收到该信号时自行终止。
在某些情况下，进程可能能够从错误中恢复并继续运行。在这些情况下，进程可以捕获信号并实现自定义恢复例程。但是，如果错误很严重，并且进程无法恢复，则进程的默认行为将是终止。
终止进程后，操作系统将释放该进程正在使用的所有资源，包括内存和其他系统资源。这确保了系统保持稳定，并且其他进程可以继续运行，而不受单个进程错误的影响。
总体而言，操作系统中的进程管理器负责检测和处理内存访问违规行为，以确保系统的稳定性和可靠性。

**How does the memory manager handle shared memory between processes in an operating system?**

共享内存是操作系统中使用的一种技术，允许多个进程访问内存的公共区域。操作系统中的内存管理器负责管理进程之间的共享内存。
当进程请求访问共享内存时，操作系统会创建一个新的共享内存段，并将其映射到请求进程的虚拟地址空间。操作系统跟踪所有可以访问共享内存段的进程。
当进程读取或写入共享内存段时，操作系统会确保所有进程之间的数据正确同步。这通常是使用信号量或互斥等同步机制来完成的。
当进程不再需要访问共享内存段时，它可以通过调用操作系统提供的系统调用来从该段分离。一旦所有进程都与共享内存段分离，操作系统就可以释放该段使用的内存。
总体而言，操作系统中的内存管理器负责创建、管理和释放进程之间的共享内存段。这允许进程高效地通信和共享数据，同时确保共享数据免受并发访问问题的影响。

**How does the memory manager keep track of which processes are using which memory regions in an operating system?**

在操作系统中，内存管理器负责跟踪哪些进程正在使用哪些内存区域。
当进程请求内存块时，内存管理器会分配该块并跟踪其地址和大小。内存管理器还创建相应的数据结构，例如页面表或内存映射，跟踪哪些物理页面属于哪些虚拟地址。
当另一个进程需要访问相同的内存区域时，内存管理器首先检查该区域是否已经分配。如果是这样，它将内存区域映射到进程的虚拟地址空间，并更新相应的数据结构以反映新的映射。
存储器管理器还可以使用附加机制来跟踪不同进程的存储器使用情况，例如：

1. 跟踪每个页面或内存区域的所有权：内存管理器可以维护每个页面或存储区域的引用计数或所有权信息。这允许它检测进程何时不再需要特定的内存区域，并可以释放它。
2. 监控共享内存段的使用：在允许多个进程共享内存的系统中，内存管理器可以跟踪哪些进程已连接到每个内存段，以及当前正在使用哪些内存段。
3. 提供内存保护：为了防止进程访问未经授权访问的内存，内存管理器可以使用硬件支持（如内存保护单元）或软件机制（如页面故障处理）来检测和处理非法内存访问。

总体而言，操作系统中的内存管理器使用各种技术来跟踪哪些进程正在使用哪些内存区域，从而确保在进程之间正确分配和共享内存。

**What is the relationship between memory management and context switching in an operating system?**

内存管理和上下文切换是操作系统的两个关键组件，它们之间密切相关。
在操作系统中，内存管理负责管理物理内存资源，包括内存的分配和释放、虚拟内存管理以及内存访问的保护。另一方面，上下文切换涉及在进程或线程被调度程序中断时保存其状态，并在稍后恢复进程或线程时恢复保存的状态。
内存管理和上下文切换之间的关系是，高效和有效的上下文切换需要适当的内存管理。当进程中断并保存其执行状态时，还必须保存有关其内存使用情况的信息，如页表条目或段描述符。稍后恢复进程时，必须将此内存相关信息与处理器的寄存器和程序计数器一起恢复。
为了实现高效和有效的上下文切换，内存管理器必须确保必要的页面或段仍然可用，并正确映射到进程的虚拟地址空间。此外，将页表和缓存等关键数据结构保留在内存中可以提高上下文切换性能。然而，这些数据结构消耗内存资源，这给内存管理器带来了有效分配和管理物理内存的压力。
总之，在操作系统中，内存管理和上下文切换是密切相关的。有效的内存管理对于实现高效和有效的上下文切换至关重要，而上下文切换则要求内存管理器确保跨上下文切换正确保存和恢复内存状态。

# VM&PRA



访问顺序:0 1 3 3 2 5 4 3 2 1

物理内存大小:3页

为了计算LRU和OPT算法的缺页异常，我们首先了解这些算法的工作原理。

LRU (Least Recently Used)算法替换最近最少使用的页。它跟踪每个页面的访问时间，并替换长时间未访问的页面。

OPT (Optimal)算法替换未来最长时间不需要的页面。它是一种理想的算法，可以预测哪个页面在最长时间内不会被访问。

下面分别使用这两种算法来计算缺页异常:

\- LRU算法:

最初，所有物理内存页都是空的。因此，前3个引用0、1和3将导致缺页异常，因为它们并不存在于物理内存中。

0 PF (1/0)

1 PF (2/1)

3 PF (3/2)

物理内存现在看起来像这样:

0 1 3

接下来，对page 3的引用已经存在于物理内存中，因此不存在缺页异常。

3 PF (3/2)

物理内存保持不变:

0 1 3

访问页2需要替换，因为物理内存已满。我们替换最近最少使用的页面，即第0页。

2 PF (4/3)

物理内存现在看起来像这样:

1 3 2

类似地，当我们替换最近最少使用的页1时，访问页5会导致另一个缺页异常。

5 PF (5/4)

物理内存现在看起来像这样:

3 2 5

访问页4需要替换，因为物理内存已满。我们替换最近最少使用的页面，即第3页。

4 PF (6/5)

物理内存现在看起来像这样:

2 5 4

访问第3页会导致缺页异常，因为在我们访问第4页时，它已经被替换了。

3 PF (7/6)

物理内存现在看起来像这样:

5 4 3

再次访问页2需要替换，因为物理内存已满。我们替换最近最少使用的页面，即第5页。

2 PF (8/7)

物理内存现在看起来像这样:

4 3 2

最后，当我们替换最近最少使用的页4时，访问页1会导致缺页异常。

1 PF (9/8)

物理内存现在看起来像这样:

3 2 1

**使用LRU算法共发生9个缺页异常。**

\- OPT算法:

该算法需要预测哪些页面在未来的最长时间内不会被访问。

0 1 3 3 2 5 4 3 2 1

最初，所有物理内存页都是空的。因此，前3个引用0、1和3将导致缺页异常，因为它们并不存在于物理内存中。

0 PF (1/0)

1 PF (2/1)

3 PF (3/2)

物理内存现在看起来像这样:

0 1 3

接下来，对page 3的引用已经存在于物理内存中，因此不存在缺页异常。

3 PF (3/2)

物理内存保持不变:

0 1 3

访问页2需要替换，因为物理内存已满。现在我们需要预测哪个页面在未来的最长时间内不会被使用。我们查看future的引用字符串，发现page 0在很长时间内都不会使用。因此，我们将第0页替换为第2页。

2 PF (4/3)

物理内存现在看起来像这样:

2 1 3

访问第5页需要替换，因为物理内存已满。现在，根据未来的页面引用字符串，我们预测页1在最长时间内不会被使用。因此，我们将第1页替换为第5页。

5 PF (5/4)

物理内存现在看起来像这样:

2 5 3

访问页4需要替换，因为物理内存已满。根据未来的页面引用字符串，我们预测页面5将在最长的时间内不会被使用。因此，我们将第5页替换为第4页。

4 PF (6/5)

物理内存现在看起来像这样:

2 4 3

访问页3不会导致缺页异常，因为它已经存在于物理内存中。

3 PF (6/5)

物理内存保持不变:

2 4 3

访问页2不会导致缺页异常，因为它已经存在于物理内存中。

物理内存现在看起来像这样:

2 4 3

访问页1需要替换，因为物理内存已满。根据未来的页面引用字符串，我们预测页面5将在最长的时间内不会被使用。因此，随便将一个替换为1

1 PF (7/6)

**在使用OPT算法时，总共有7个缺页异常。**

因此，LRU算法导致9个缺页异常，OPT算法导致7个缺页异常。



**一台32位地址的计算机使用两级页表。虚拟地址被分成一个9位的顶级页表字段，一个11位的二级页表字段，以及一个偏移量。页有多大，地址空间里有多少个？**

\1. 32位地址的计算机,每个地址32位,范围0x00000000~0xFFFFFFFF。

2. 两级页表。第一级页表的字段长度为9位,第二级页表字段长度为11位。9 + 11 + 偏移量 = 32位。

3. 3. 想要求:页大小和地址空间大小。

   3. 根据以上信息,可以算出:1. 9位的第一级页表字段范围为0~511,可以表示512个条目。2. 11位的第二级页表字段范围为0~2047,每个第一级页表条目可以表示2048个第二级页表条目。3. 偏移量字段长度为32 - (9 + 11) = 12位。每个第二级页表条目表示的页大小为2^12 = 4KB。4. 整个32位地址空间包含512 * 2048 = 1,048,576个4KB大小的页。5. 所以地址空间总大小为4KB * 1,048,576 = 4GB。总结:
      页大小:4KB
      地址空间大小:4GB两级页表机制允许地址空间被划分为两级,第一级由512个页目录项指向2048个第二级页表,第二级页表进一步指向4KB的页,这种结构实现了两级的间接寻址,有效地扩展和管理了4GB的地址空间。两级页表在早期Intel的80386处理器中采用,这也是现代操作系统中最常见和基本的页表结构形式,理解其原理对学习操作系统的虚拟内存管理有很好的帮助。

偏移量=32-9-11 = 12（位），所以页面大小为：2^12 = 4KB，页面数为：2^20。

**对于以下每个十进制的虚拟地址，计算4KB页和8KB页的虚拟页号和偏移量： 0x20000, 0x32768, 0x60000**

地址:0x20000 4KB页:
虚拟页号:0x20(十进制32)
偏移量:0x0008KB页:
虚拟页号:0x10(十进制16)
偏移量:0x2000地址:0x327684KB页: 
虚拟页号:0x32(十进制50) 
偏移量:0x7688KB页:
虚拟页号:0x20(十进制32)
偏移量:0x768地址:0x600004KB页:
虚拟页号:0x60(十进制96)
偏移量:0x0008KB页:
虚拟页号:0x30(十进制48)
偏移量:0x0000总结:
对4KB页:
0x20000 -> 虚拟页号:0x20,偏移量:0x000
0x32768 -> 虚拟页号:0x32,偏移量:0x768
0x60000 -> 虚拟页号:0x60,偏移量:0x000对8KB页:
0x20000 -> 虚拟页号:0x10,偏移量:0x2000 
0x32768 -> 虚拟页号:0x20,偏移量:0x768
0x60000 -> 虚拟页号:0x30,偏移量:0x0000

**如果将FIFO页面罝换算法用到4个页框和8个页面上，若初始时页框为空，访问字符串为0172327103，请问会发生多少次缺页中断？如果使用LRU算法呢？**

FIFO的页框如下：



x0172333300



xx017222233



xxx01777722



xxxx0111177



LRU的页框如下：



x0172327103



xx017232710



xxx01773271



xxxx0111327



FIFO发生6次缺页中断，LRU发生7次缺页中断。

**What is the difference between a physical address and a virtual address?**

在操作系统中，物理地址和虚拟地址有很大的区别。

物理地址是指计算机内存中的实际存储位置，可以被CPU直接访问。每个字节都有一个唯一的物理地址，CPU使用物理地址来读取和写入内存中的数据。

而虚拟地址则是由程序生成的一种抽象地址，它与物理地址是分离的。程序使用虚拟地址来引用内存中的数据，而不需要知道数据在物理内存中的真实位置。操作系统将虚拟地址映射到物理地址，从而使程序可以访问内存中的数据。

虚拟内存技术利用了虚拟地址的概念，在物理内存和硬盘之间创建了一个抽象层，使得程序能够访问比物理内存更大的地址空间。同时，虚拟内存也可以让多个程序共享同一个物理内存，从而提高系统效率。

总之，物理地址和虚拟地址是操作系统中非常重要的概念，它们的不同特点决定了它们在内存管理中的不同作用。

**What is thrashing and how can an OS prevent it?**

thrashing 是指操作系统一直处于分页状态，这意味着它在物理内存和磁盘之间交换页面的时间比执行实际指令的时间更多。这可能导致系统性能极其缓慢，使计算机实际上无法使用。

thrashing 通常发生在系统负载过重或没有足够的物理内存来支持所有运行的进程时。当系统耗尽可用的物理内存时，它开始在内存和磁盘之间来回移动页面。如果系统在这方面花费太多时间，它将没有足够的时间执行实际的指令，从而导致thrashing 。

为了防止thrashing ，操作系统可以采用几种不同的策略。首先，它可以通过不允许太多进程并发地占用物理内存来避免过度分配内存。其次，它可以使用一种分页算法，对频繁访问的页面进行优先级排序，避免将这些页面换出到磁盘。第三，它可以增加系统中的物理内存数量，从而使进程有更多的可用空间驻留在内存中。

除了这些策略之外，操作系统还可以实现一些技术，例如预取在不久的将来可能会用到的页，识别不再使用的页以释放物理内存中的空间。总的来说，防止thrashing 的关键是确保有足够的物理内存可供系统高效地执行其任务。